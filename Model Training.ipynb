{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epycmNPr0I3d"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dwn1PtIp4058"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteo/.pyenv/versions/3.11.2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT = os.path.join(\"./\")\n",
    "sys.path.append(ROOT + \"lib\")\n",
    "\n",
    "from helpers import *\n",
    "from sourceset import SourceSet\n",
    "from general_trainer import compete\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2917,
     "status": "ok",
     "timestamp": 1698123021083,
     "user": {
      "displayName": "Matteo",
      "userId": "07662895649750931658"
     },
     "user_tz": 420
    },
    "id": "0o5bi2-e_n2z",
    "outputId": "1792997b-1077-4089-b8ae-b1b818428b61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<sourceset.SourceSet at 0x13e846190>, <sourceset.SourceSet at 0x13cfd97d0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(ROOT + \"processed_datasets/data_train.pt\", \"rb\") as f:\n",
    "  data_train = torch.load(f, map_location=device)\n",
    "with open(ROOT + \"processed_datasets/data_valid.pt\", \"rb\") as f:\n",
    "  data_valid = torch.load(f, map_location=device)\n",
    "\n",
    "data_train, data_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4Zzbo31tIp"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gd-gDX5vfwTG"
   },
   "outputs": [],
   "source": [
    "class FluxAnomalyPredictionLSTMDEPRECATED(nn.Module):\n",
    "  def __init__(self, stride, dropout, bn=False, features=6, residual=0, out=4):\n",
    "    super().__init__()\n",
    "\n",
    "    self.stride = stride\n",
    "    self.features = features\n",
    "    self.residual = residual\n",
    "    self.lstm_hidden_state_features = 14 # Needed for batchnorm 3, so defined here\n",
    "\n",
    "    # Need Dropouts, Activation fns\n",
    "    self.he = lambda x: nn.init.kaiming_normal_(x, nonlinearity='relu')\n",
    "    self.av = nn.ReLU()\n",
    "    self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    self.batchnorm1 = lambda x: x\n",
    "    self.batchnorm2 = lambda x: x\n",
    "    self.batchnorm3 = lambda x: x\n",
    "\n",
    "    if bn:\n",
    "      self.batchnorm1 = nn.BatchNorm1d(features)\n",
    "      self.batchnorm2 = nn.BatchNorm1d(features)\n",
    "      self.batchnorm3 = nn.BatchNorm1d(self.lstm_hidden_state_features)\n",
    "\n",
    "      self.bn1 = lambda x: torch.transpose(self.batchnorm1(torch.transpose(x,1,2)),1,2)\n",
    "      self.bn2 = lambda x: torch.transpose(self.batchnorm2(torch.transpose(x,1,2)),1,2)\n",
    "      self.bn3 = lambda x: self.batchnorm3(x)\n",
    "    else:\n",
    "      self.bn1 = lambda x: x\n",
    "      self.bn2 = lambda x: x\n",
    "      self.bn3 = lambda x: x\n",
    "\n",
    "\n",
    "    # Step 1\n",
    "\n",
    "    # 5 x 64 X 64 x 5 -diag-> R^5 Vector\n",
    "    self.conv_kernels_64d = nn.ParameterList([self.he(torch.randn(features, 64)) for i in range(8)])\n",
    "    self.conv_biases_64d = nn.ParameterList([torch.randn(features) for i in range(8)])\n",
    "\n",
    "    self.conv_kernel_16d = nn.Parameter(self.he(torch.randn(features, 16)))\n",
    "    self.conv_bias_16d = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.conv_kernel_8d = nn.Parameter(self.he(torch.randn(features,8)))\n",
    "    self.conv_bias_8d = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.four_max_pool = nn.MaxPool1d(4)\n",
    "\n",
    "    # Step 1.5\n",
    "    # self.certainty_fc_1 = nn.Parameter(torch.zeros(2,3))\n",
    "    # self.certainty_fc_2 = nn.Parameter(torch.zeros(3,1))\n",
    "\n",
    "    # Step 2\n",
    "    self.widechannel_conv_kernel = nn.Parameter(self.he(torch.randn(features, 3)))\n",
    "    self.widechannel_conv_bias = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.midchannel_conv_kernel = nn.Parameter(self.he(torch.randn(features, 3)))\n",
    "    self.midchannel_conv_bias = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.narrowchannel_conv_kernel = nn.Parameter(self.he(torch.randn(features, 3)))\n",
    "    self.narrowchannel_conv_bias = nn.Parameter(torch.randn(features))\n",
    "\n",
    "\n",
    "    # Step 3\n",
    "    self.pair_max_pool = nn.MaxPool1d(2)\n",
    "    # Then concat all vectors into v \\in R^24\n",
    "\n",
    "    # Step 3.5 Residual connection\n",
    "    self.__avgpool__ = nn.AvgPool1d(4, stride=4)\n",
    "\n",
    "    # Step 4\n",
    "    self.hidden_fc_1 = nn.Linear(3*4*features,3*2*features)\n",
    "    self.hidden_fc_2 = nn.Linear(3*2*features,2*features)\n",
    "\n",
    "    # Step 5\n",
    "    # LSTM\n",
    "\n",
    "    self.lstm_in_features = 2*self.features\n",
    "\n",
    "    self.lstm = nn.LSTM(self.lstm_in_features, self.lstm_hidden_state_features)\n",
    "\n",
    "    # Step 6\n",
    "    # Softmax\n",
    "\n",
    "    self.to_out_1 = nn.Linear(self.lstm_hidden_state_features, 7)\n",
    "    self.to_out_2 = nn.Linear(7, out, bias=False)\n",
    "\n",
    "    self.prob = nn.Softmax(dim=1)\n",
    "\n",
    "    for param in self.parameters():\n",
    "      if len(param.shape) >= 2:\n",
    "        param = self.he(param)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = Batches X Time X Channels\n",
    "\n",
    "    N = x.shape[0] # Batches\n",
    "    T = x.shape[1] # Time\n",
    "\n",
    "\n",
    "    # Step 1\n",
    "    # Cursory vision convolution\n",
    "    pad_amt = 40\n",
    "\n",
    "    pad = torch.zeros(N, pad_amt, self.features).to(device)\n",
    "\n",
    "    padded_x = torch.cat((pad, torch.cat((x, pad), dim=1)), dim=1) # catting in time\n",
    "\n",
    "    # padded_x.requires_grad = True\n",
    "\n",
    "    window_centers = [] # AKA 64d Convolve Centers\n",
    "\n",
    "    n=0 # strides\n",
    "    while(True):\n",
    "      next_center = pad_amt + 1 + n * self.stride\n",
    "      if next_center > (pad_amt + T + 1): # If our center isnt in real data\n",
    "        break;\n",
    "      else:\n",
    "        window_centers.append(next_center)\n",
    "        n += 1\n",
    "\n",
    "    window_starts = [self.start_and_end_from_center((64 - 1)/2, i)[0] for i in window_centers]\n",
    "    window_ends = [self.start_and_end_from_center((64 - 1)/2, i)[1] for i in window_centers]\n",
    "\n",
    "    midchannel_centers = []\n",
    "    narrowchannel_centers = []\n",
    "    for start in window_starts:\n",
    "      for n in range(8): # 8 strides of 8 -> 64 units\n",
    "        midchannel_centers.append(start + n * 8)\n",
    "\n",
    "      for n in range(32): # 32 strides of 2 -> 64 units\n",
    "        narrowchannel_centers.append(start + n * 2)\n",
    "\n",
    "\n",
    "    wide_convs = []\n",
    "    mid_convs = []\n",
    "    narrow_convs = []\n",
    "\n",
    "    for i in window_centers:\n",
    "      for j in range(8): # Hard coded 8 here\n",
    "        K = self.conv_kernels_64d[j]\n",
    "        B = self.conv_biases_64d[j].repeat(N,1) # Repeat here\n",
    "        conv = self.convolve(K, padded_x, i) # Features x T\n",
    "        conv += B\n",
    "        wide_convs.append(conv)\n",
    "\n",
    "    for i in midchannel_centers:\n",
    "      conv = self.convolve(self.conv_kernel_16d, padded_x, i)\n",
    "      conv += self.conv_bias_16d.repeat(N,1)\n",
    "      mid_convs.append(conv)\n",
    "\n",
    "    for i in narrowchannel_centers:\n",
    "      conv = self.convolve(self.conv_kernel_8d, padded_x, i)\n",
    "      conv += self.conv_bias_8d.repeat(N,1)\n",
    "      narrow_convs.append(conv)\n",
    "\n",
    "    wide_convs = torch.stack(wide_convs, dim=1).to(device)\n",
    "    mid_convs = torch.stack(mid_convs, dim=1).to(device)\n",
    "    narrow_convs = torch.stack(narrow_convs, dim=1).to(device)\n",
    "\n",
    "    narrow_convs = self.four_max_pool(torch.transpose(narrow_convs, 1,2)) # Inp = N x C x L now\n",
    "    narrow_convs = torch.transpose(narrow_convs,1,2) # Back to N x L x C\n",
    "\n",
    "\n",
    "    wide_convs = self.bn1(self.av(wide_convs))\n",
    "    mid_convs = self.bn1(self.av(mid_convs))\n",
    "    narrow_convs = self.bn1(self.av(narrow_convs))\n",
    "\n",
    "    residual_vectors = self.get_residual_vectors(narrow_convs, mid_convs, wide_convs)\n",
    "\n",
    "\n",
    "    wide_convs = 5 * self.drop(wide_convs)\n",
    "    mid_convs = self.drop(mid_convs)\n",
    "    narrow_convs = self.drop(narrow_convs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # convs = N x L x C\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #####\n",
    "    # STEP 1.5\n",
    "    # Compress our 5vectors to 3 vectors, combining mag and uncertainty indices\n",
    "\n",
    "    # if not (wide_convs.shape[1] == mid_convs.shape[1] == narrow_convs.shape[1]):\n",
    "    #   raise Exception(\"Step 1 output mismatch\")\n",
    "\n",
    "\n",
    "    # compacting_fc = lambda x: self.gelu(torch.matmul(self.gelu(torch.matmul(x, self.certainty_fc_1)), self.certainty_fc_2))\n",
    "\n",
    "    # wide_col_0 = compacting_fc(wide_convs[:, 0:2])\n",
    "    # wide_col_1 = compacting_fc(wide_convs[:, 2:4])\n",
    "\n",
    "    # mid_col_0 = compacting_fc(mid_convs[:, 0:2])\n",
    "    # mid_col_1 = compacting_fc(mid_convs[:, 2:4])\n",
    "\n",
    "    # narrow_col_0 = compacting_fc(mid_convs[:, 0:2])\n",
    "    # narrow_col_1 = compacting_fc(mid_convs[:, 2:4])\n",
    "\n",
    "    # wide_convs = torch.stack((wide_col_0, wide_col_1, wide_convs[:, -1].unsqueeze(1)), dim=1)\n",
    "    # mid_convs = torch.stack((mid_col_0, mid_col_1, mid_convs[:, -1].unsqueeze(1)), dim=1)\n",
    "    # narrow_convs = torch.stack((narrow_col_0, narrow_col_1, narrow_convs[:, -1].unsqueeze(1)), dim=1)\n",
    "\n",
    "    # wide_convs = wide_convs.squeeze()\n",
    "    # mid_convs = mid_convs.squeeze()\n",
    "    # narrow_convs = narrow_convs.squeeze()\n",
    "\n",
    "\n",
    "    #####\n",
    "    # STEP 2\n",
    "    # Second Convolution\n",
    "\n",
    "    pad_amt = 10\n",
    "    stride = 1\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for x in (wide_convs, mid_convs, narrow_convs):\n",
    "      T = x.shape[1]\n",
    "      ker = None\n",
    "      bias = None\n",
    "      if len(results) == 0:\n",
    "        ker = self.widechannel_conv_kernel\n",
    "        bias = self.widechannel_conv_bias.repeat(N, 1)\n",
    "\n",
    "      elif len(results) == 1:\n",
    "        ker = self.midchannel_conv_kernel\n",
    "        bias = self.midchannel_conv_bias.repeat(N, 1)\n",
    "\n",
    "      elif len(results) == 2:\n",
    "        ker = self.narrowchannel_conv_kernel\n",
    "        bias = self.narrowchannel_conv_bias.repeat(N, 1)\n",
    "\n",
    "\n",
    "      pad = torch.zeros(N, pad_amt, self.features).to(device)\n",
    "      padded_x = torch.cat((pad, torch.cat((x, pad), dim=1)), dim=1)\n",
    "\n",
    "\n",
    "      result = []\n",
    "\n",
    "      next = pad_amt\n",
    "      while next <= (pad_amt + T - 1):\n",
    "        v = bias + self.convolve(ker, padded_x, next)\n",
    "        result.append(v)\n",
    "        next += stride\n",
    "\n",
    "\n",
    "      results.append(torch.stack(result, dim=1))\n",
    "\n",
    "\n",
    "    wide_convs = self.bn2(self.av(results[0]))\n",
    "    mid_convs = self.bn2(self.av(results[1]))\n",
    "    narrow_convs = self.bn2(self.av(results[2]))\n",
    "\n",
    "    wide_convs = self.drop(wide_convs)\n",
    "    mid_convs = self.drop(mid_convs)\n",
    "    narrow_convs = self.drop(narrow_convs)\n",
    "\n",
    "\n",
    "\n",
    "    ####\n",
    "    # Step 2.5\n",
    "    # Max Pooling Pairs\n",
    "    wide_convs = self.pair_max_pool(torch.transpose(wide_convs, 1,2)) # to N x C x L\n",
    "    mid_convs = self.pair_max_pool(torch.transpose(mid_convs, 1,2))\n",
    "    narrow_convs = self.pair_max_pool(torch.transpose(narrow_convs, 1,2))\n",
    "\n",
    "    wide_convs = torch.transpose(wide_convs, 1,2) # to N x L x C\n",
    "    mid_convs = torch.transpose(mid_convs, 1,2)\n",
    "    narrow_convs = torch.transpose(narrow_convs, 1,2)\n",
    "\n",
    "\n",
    "    ####\n",
    "    # Step 3\n",
    "\n",
    "    # Now each original window reigon is each corresponding 2 rows from all 3 tensors\n",
    "    # 2 rows evenly divides all possible resulting lengths\n",
    "\n",
    "    if not (wide_convs.shape[1] == mid_convs.shape[1] == narrow_convs.shape[1]):\n",
    "      raise Exception(\"Step 3 output mismatch\")\n",
    "\n",
    "\n",
    "    hidden = []\n",
    "    L = wide_convs.shape[1]\n",
    "\n",
    "    for n in range(L // 4): # Now each sliding window corresponds to 2 rows\n",
    "      wide = wide_convs[:, 4*n:4*n + 4].reshape(N, 4*self.features)\n",
    "      mid = mid_convs[:, 4*n:4*n + 4].reshape(N, 4*self.features)\n",
    "      narrow = narrow_convs[:, 4*n:4*n + 4].reshape(N, 4*self.features)\n",
    "\n",
    "      flat = torch.cat((wide, mid, narrow), dim=1).to(device)\n",
    "      if flat.shape[0] != N or flat.shape[1] != 3*4*self.features:\n",
    "        raise Exception(\"Flat shape err\")\n",
    "\n",
    "      layer_1 = self.hidden_fc_1(flat)\n",
    "      layer_1 = self.av(layer_1)\n",
    "\n",
    "      # RESIDUAL CONNECTION !\n",
    "      res = torch.autograd.Variable(self.residual * residual_vectors[n])\n",
    "      layer_1 = (1-self.residual) * layer_1\n",
    "      layer_1 = layer_1 + res\n",
    "\n",
    "\n",
    "      layer_2 = self.hidden_fc_2(layer_1)\n",
    "      layer_2 = self.av(layer_2)\n",
    "\n",
    "      hidden.append(layer_2)\n",
    "\n",
    "\n",
    "    hidden = torch.stack(hidden, dim=0) # Results in L x N x Hidden\n",
    "\n",
    "    hidden = self.drop(hidden)\n",
    "\n",
    "\n",
    "    ####\n",
    "    # Step 6: LSTM\n",
    "\n",
    "    _, (final_hidden_state, c_n) = self.lstm(hidden)\n",
    "\n",
    "    final_hidden_state = self.av(final_hidden_state.squeeze())\n",
    "    final_hidden_state = self.bn3(final_hidden_state)\n",
    "\n",
    "    final_hidden_state = self.drop(final_hidden_state)\n",
    "\n",
    "\n",
    "    final_layer = self.to_out_1(final_hidden_state)\n",
    "    final_layer = self.av(final_layer)\n",
    "\n",
    "    final_layer = self.drop(final_layer)\n",
    "\n",
    "    final_layer = self.to_out_2(final_layer)\n",
    "\n",
    "    classes = self.prob(final_layer)\n",
    "\n",
    "\n",
    "    return F.log_softmax(final_layer, dim=1)\n",
    "\n",
    "\n",
    "  def start_and_end_from_center(self, width, i):\n",
    "    start = i - np.ceil(width)\n",
    "    end = i + np.floor(width) + 1\n",
    "    return (int(start), int(end))\n",
    "\n",
    "\n",
    "\n",
    "  def convolve(self, Kernel, Data, i):\n",
    "\n",
    "    # i is center index so we take equal on either side\n",
    "    T = Kernel.shape[1]\n",
    "    each_side = (T - 1) / 2\n",
    "\n",
    "    # Moves it backwards 1 if kernel is even\n",
    "    start, end = self.start_and_end_from_center(each_side, i)\n",
    "\n",
    "    adj_data = Data[:, start:end, :]\n",
    "\n",
    "    # So now K is 5 x L and adj_data is N x L X 5\n",
    "\n",
    "\n",
    "    N = adj_data.shape[0]\n",
    "\n",
    "    m = torch.bmm(Kernel.repeat(N,1,1), adj_data) # bij, bjk -> bik Slightly faster than einsum\n",
    "\n",
    "    # m = torch.einsum(\"ij, bjk -> bik\", Kernel, adj_data) #identical batch matmul\n",
    "\n",
    "    diag = torch.einsum(\"bii->bi\", m)\n",
    "    # diags = []\n",
    "    # for res in m:\n",
    "    #   diags.append(torch.diag(res))\n",
    "\n",
    "\n",
    "    return diag\n",
    "\n",
    "  def get_residual_vectors(self, narrow, mid, wide):\n",
    "    # Should be N x 8m x Features\n",
    "    # print(narrow.shape, mid.shape, wide.shape)\n",
    "    N = wide.shape[0]\n",
    "    L = wide.shape[1]\n",
    "\n",
    "    apply_avg = lambda matrix: torch.transpose(self.__avgpool__(torch.transpose(matrix, 1,2)), 1,2)\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    if L % 8:\n",
    "      raise Exception(\"Something went wrong, convs are not mult of 8\")\n",
    "    for i in range(L // 8):\n",
    "\n",
    "      n = torch.autograd.Variable(narrow[:, 8*i:8*(i+1), :])\n",
    "      m = torch.autograd.Variable(mid[:, 8*i:8*(i+1), :])\n",
    "      w = torch.autograd.Variable(wide[:, 8*i:8*(i+1), :])\n",
    "\n",
    "      n = apply_avg(n)\n",
    "      m = apply_avg(m)\n",
    "      w = apply_avg(w)\n",
    "\n",
    "      n = n.reshape(N, 2*self.features)\n",
    "      m = m.reshape(N, 2*self.features)\n",
    "      w = w.reshape(N, 2*self.features)\n",
    "      # gives N x 2 * Features\n",
    "\n",
    "      vectors.append(torch.autograd.Variable(torch.cat((n,m,w), dim=1))) # N x 6 * features\n",
    "\n",
    "    return vectors\n",
    "\n",
    "class FluxAnomalyPredictionTF(nn.Module):\n",
    "  def __init__(self, stride, dropout, bn=False, features=3, residual=0, out=4):\n",
    "    super().__init__()\n",
    "\n",
    "    self.stride = stride\n",
    "    self.features = features\n",
    "    self.residual = residual\n",
    "    self.transformer_hidden_dim = 2*features # Needed for batchnorm 3, so defined here\n",
    "\n",
    "    # Need Dropouts, Activation fns\n",
    "    self.he = lambda x: nn.init.kaiming_normal_(x, nonlinearity='relu')\n",
    "    self.av = nn.ReLU()\n",
    "    self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    self.batchnorm1 = lambda x: x\n",
    "    self.batchnorm2 = lambda x: x\n",
    "    self.batchnorm3 = lambda x: x\n",
    "\n",
    "    if bn:\n",
    "      self.batchnorm1 = nn.BatchNorm1d(features)\n",
    "      self.batchnorm2 = nn.BatchNorm1d(features)\n",
    "      self.batchnorm3 = nn.BatchNorm1d(self.transformer_hidden_dim)\n",
    "\n",
    "      self.bn1 = lambda x: torch.transpose(self.batchnorm1(torch.transpose(x,1,2)),1,2)\n",
    "      self.bn2 = lambda x: torch.transpose(self.batchnorm2(torch.transpose(x,1,2)),1,2)\n",
    "      self.bn3 = lambda x: self.batchnorm3(x)\n",
    "    else:\n",
    "      self.bn1 = lambda x: x\n",
    "      self.bn2 = lambda x: x\n",
    "      self.bn3 = lambda x: x\n",
    "\n",
    "\n",
    "    # Step 1\n",
    "\n",
    "    # 5 x 64 X 64 x 5 -diag-> R^5 Vector\n",
    "    self.conv_kernels_64d = nn.ParameterList([self.he(torch.randn(features, 64)) for i in range(8)])\n",
    "    self.conv_biases_64d = nn.ParameterList([torch.randn(features) for i in range(8)])\n",
    "\n",
    "    self.conv_kernel_16d = nn.Parameter(self.he(torch.randn(features, 16)))\n",
    "    self.conv_bias_16d = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.conv_kernel_8d = nn.Parameter(self.he(torch.randn(features,8)))\n",
    "    self.conv_bias_8d = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.four_max_pool = nn.MaxPool1d(4)\n",
    "\n",
    "    # Step 1.5\n",
    "    # self.certainty_fc_1 = nn.Parameter(torch.zeros(2,3))\n",
    "    # self.certainty_fc_2 = nn.Parameter(torch.zeros(3,1))\n",
    "\n",
    "    # Step 2\n",
    "    self.widechannel_conv_kernel = nn.Parameter(self.he(torch.randn(features, 3)))\n",
    "    self.widechannel_conv_bias = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.midchannel_conv_kernel = nn.Parameter(self.he(torch.randn(features, 3)))\n",
    "    self.midchannel_conv_bias = nn.Parameter(torch.randn(features))\n",
    "\n",
    "    self.narrowchannel_conv_kernel = nn.Parameter(self.he(torch.randn(features, 3)))\n",
    "    self.narrowchannel_conv_bias = nn.Parameter(torch.randn(features))\n",
    "\n",
    "\n",
    "    # Step 3\n",
    "    self.pair_max_pool = nn.MaxPool1d(2)\n",
    "    # Then concat all vectors into v \\in R^24\n",
    "\n",
    "    # Step 3.5 Residual connection\n",
    "    self.__avgpool__ = nn.AvgPool1d(4, stride=4)\n",
    "\n",
    "    # Step 4\n",
    "    self.hidden_fc_1 = nn.Linear(3*4*features, 3*2*features)\n",
    "    self.hidden_fc_2 = nn.Linear(3*2*features, self.transformer_hidden_dim)\n",
    "\n",
    "    # Step 5\n",
    "    # Transformer\n",
    "    self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=self.transformer_hidden_dim, nhead=self.features)\n",
    "    self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, 2)\n",
    "\n",
    "\n",
    "    # Step 6\n",
    "    # Softmax\n",
    "\n",
    "    self.to_out_1 = nn.Linear(self.transformer_hidden_dim, 7)\n",
    "    self.to_out_2 = nn.Linear(7, out, bias=False)\n",
    "\n",
    "    self.prob = nn.Softmax(dim=1)\n",
    "\n",
    "    for param in self.parameters():\n",
    "      if len(param.shape) >= 2:\n",
    "        param = self.he(param)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x = Batches X Time X Channels\n",
    "\n",
    "    N = x.shape[0] # Batches\n",
    "    T = x.shape[1] # Time\n",
    "\n",
    "    if x.shape[2] != self.features:\n",
    "      raise Exception(\"Feature dimension mismatch\")\n",
    "\n",
    "    # Step 1\n",
    "    # Cursory vision convolution\n",
    "    pad_amt = 40\n",
    "\n",
    "    pad = torch.zeros(N, pad_amt, self.features).to(device)\n",
    "  \n",
    "\n",
    "    padded_x = torch.cat((pad, torch.cat((x, pad), dim=1)), dim=1) # catting in time\n",
    "\n",
    "    # padded_x.requires_grad = True\n",
    "\n",
    "    window_centers = [] # AKA 64d Convolve Centers\n",
    "\n",
    "    n=0 # strides\n",
    "    while(True):\n",
    "      next_center = pad_amt + 1 + n * self.stride\n",
    "      if next_center > (pad_amt + T + 1): # If our center isnt in real data\n",
    "        break;\n",
    "      else:\n",
    "        window_centers.append(next_center)\n",
    "        n += 1\n",
    "\n",
    "    window_starts = [self.start_and_end_from_center((64 - 1)/2, i)[0] for i in window_centers]\n",
    "    window_ends = [self.start_and_end_from_center((64 - 1)/2, i)[1] for i in window_centers]\n",
    "\n",
    "    midchannel_centers = []\n",
    "    narrowchannel_centers = []\n",
    "    for start in window_starts:\n",
    "      for n in range(8): # 8 strides of 8 -> 64 units\n",
    "        midchannel_centers.append(start + n * 8)\n",
    "\n",
    "      for n in range(32): # 32 strides of 2 -> 64 units\n",
    "        narrowchannel_centers.append(start + n * 2)\n",
    "\n",
    "\n",
    "    wide_convs = []\n",
    "    mid_convs = []\n",
    "    narrow_convs = []\n",
    "\n",
    "    for i in window_centers:\n",
    "      for j in range(8): # Hard coded 8 here\n",
    "        K = self.conv_kernels_64d[j]\n",
    "        B = self.conv_biases_64d[j].repeat(N,1) # Repeat here\n",
    "        conv = self.convolve(K, padded_x, i) # Features x T\n",
    "        conv += B\n",
    "        wide_convs.append(conv)\n",
    "\n",
    "    for i in midchannel_centers:\n",
    "      conv = self.convolve(self.conv_kernel_16d, padded_x, i)\n",
    "      conv += self.conv_bias_16d.repeat(N,1)\n",
    "      mid_convs.append(conv)\n",
    "\n",
    "    for i in narrowchannel_centers:\n",
    "      conv = self.convolve(self.conv_kernel_8d, padded_x, i)\n",
    "      conv += self.conv_bias_8d.repeat(N,1)\n",
    "      narrow_convs.append(conv)\n",
    "\n",
    "    wide_convs = torch.stack(wide_convs, dim=1).to(device)\n",
    "    mid_convs = torch.stack(mid_convs, dim=1).to(device)\n",
    "    narrow_convs = torch.stack(narrow_convs, dim=1).to(device)\n",
    "\n",
    "    narrow_convs = self.four_max_pool(torch.transpose(narrow_convs, 1,2)) # Inp = N x C x L now\n",
    "    narrow_convs = torch.transpose(narrow_convs,1,2) # Back to N x L x C\n",
    "\n",
    "\n",
    "    wide_convs = self.bn1(self.av(wide_convs))\n",
    "    mid_convs = self.bn1(self.av(mid_convs))\n",
    "    narrow_convs = self.bn1(self.av(narrow_convs))\n",
    "\n",
    "    residual_vectors = self.get_residual_vectors(narrow_convs, mid_convs, wide_convs)\n",
    "\n",
    "\n",
    "    wide_convs = 5 * self.drop(wide_convs)\n",
    "    mid_convs = self.drop(mid_convs)\n",
    "    narrow_convs = self.drop(narrow_convs)\n",
    "\n",
    "\n",
    "\n",
    "    #####\n",
    "    # STEP 2\n",
    "    # Second Convolution\n",
    "\n",
    "    pad_amt = 10\n",
    "    stride = 1\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for x in (wide_convs, mid_convs, narrow_convs):\n",
    "      T = x.shape[1]\n",
    "      ker = None\n",
    "      bias = None\n",
    "      if len(results) == 0:\n",
    "        ker = self.widechannel_conv_kernel\n",
    "        bias = self.widechannel_conv_bias.repeat(N, 1)\n",
    "\n",
    "      elif len(results) == 1:\n",
    "        ker = self.midchannel_conv_kernel\n",
    "        bias = self.midchannel_conv_bias.repeat(N, 1)\n",
    "\n",
    "      elif len(results) == 2:\n",
    "        ker = self.narrowchannel_conv_kernel\n",
    "        bias = self.narrowchannel_conv_bias.repeat(N, 1)\n",
    "\n",
    "\n",
    "      pad = torch.zeros(N, pad_amt, self.features).to(device)\n",
    "      padded_x = torch.cat((pad, torch.cat((x, pad), dim=1)), dim=1)\n",
    "\n",
    "\n",
    "      result = []\n",
    "\n",
    "      next = pad_amt\n",
    "      while next <= (pad_amt + T - 1):\n",
    "        v = bias + self.convolve(ker, padded_x, next)\n",
    "        result.append(v)\n",
    "        next += stride\n",
    "\n",
    "\n",
    "      results.append(torch.stack(result, dim=1))\n",
    "\n",
    "\n",
    "    wide_convs = self.bn2(self.av(results[0]))\n",
    "    mid_convs = self.bn2(self.av(results[1]))\n",
    "    narrow_convs = self.bn2(self.av(results[2]))\n",
    "\n",
    "    wide_convs = self.drop(wide_convs)\n",
    "    mid_convs = self.drop(mid_convs)\n",
    "    narrow_convs = self.drop(narrow_convs)\n",
    "\n",
    "\n",
    "\n",
    "    ####\n",
    "    # Step 2.5\n",
    "    # Max Pooling Pairs\n",
    "    wide_convs = self.pair_max_pool(torch.transpose(wide_convs, 1,2)) # to N x C x L\n",
    "    mid_convs = self.pair_max_pool(torch.transpose(mid_convs, 1,2))\n",
    "    narrow_convs = self.pair_max_pool(torch.transpose(narrow_convs, 1,2))\n",
    "\n",
    "    wide_convs = torch.transpose(wide_convs, 1,2) # to N x L x C\n",
    "    mid_convs = torch.transpose(mid_convs, 1,2)\n",
    "    narrow_convs = torch.transpose(narrow_convs, 1,2)\n",
    "\n",
    "\n",
    "    ####\n",
    "    # Step 3\n",
    "\n",
    "    # Now each original window reigon is each corresponding 2 rows from all 3 tensors\n",
    "    # 2 rows evenly divides all possible resulting lengths\n",
    "\n",
    "    if not (wide_convs.shape[1] == mid_convs.shape[1] == narrow_convs.shape[1]):\n",
    "      raise Exception(\"Step 3 output mismatch\")\n",
    "\n",
    "\n",
    "    hidden = []\n",
    "    L = wide_convs.shape[1]\n",
    "\n",
    "    for n in range(L // 4): # Now each sliding window corresponds to 2 rows\n",
    "      wide = wide_convs[:, 4*n:4*n + 4].reshape(N, 4*self.features)\n",
    "      mid = mid_convs[:, 4*n:4*n + 4].reshape(N, 4*self.features)\n",
    "      narrow = narrow_convs[:, 4*n:4*n + 4].reshape(N, 4*self.features)\n",
    "\n",
    "      flat = torch.cat((wide, mid, narrow), dim=1).to(device)\n",
    "      if flat.shape[0] != N or flat.shape[1] != 3*4*self.features:\n",
    "        raise Exception(\"Flat shape err\")\n",
    "\n",
    "      layer_1 = self.hidden_fc_1(flat)\n",
    "      layer_1 = self.av(layer_1)\n",
    "\n",
    "      # RESIDUAL CONNECTION !\n",
    "      res = torch.autograd.Variable(self.residual * residual_vectors[n])\n",
    "      layer_1 = (1-self.residual) * layer_1\n",
    "      layer_1 = layer_1 + res\n",
    "\n",
    "      layer_2 = self.hidden_fc_2(layer_1)\n",
    "      layer_2 = self.av(layer_2)\n",
    "\n",
    "      hidden.append(layer_2)\n",
    "\n",
    "    hidden = torch.stack(hidden, dim=0) # Results in (Divided L) x N x Hidden\n",
    "    hidden = self.drop(hidden)\n",
    "\n",
    "\n",
    "    seq = self.transformer_encoder(hidden) # same shape as hidden\n",
    "\n",
    "\n",
    "\n",
    "    transformed = nn.AvgPool1d(seq.shape[0])(torch.transpose(seq,0,2)) # Pooling happens on last dim\n",
    "    transformed = torch.squeeze(transformed, dim=2)\n",
    "    transformed = torch.transpose(transformed, 0, 1) # Should be N x Hidden\n",
    "\n",
    "    transformed = self.av(transformed)\n",
    "    transformed = self.bn3(transformed)\n",
    "    transformed = self.drop(transformed)\n",
    "\n",
    "    final_layer = self.to_out_1(transformed)\n",
    "    final_layer = self.av(final_layer)\n",
    "    final_layer = self.drop(final_layer)\n",
    "    final_layer = self.to_out_2(final_layer)\n",
    "\n",
    "    # classes = self.prob(final_layer) # Dont use, use CEL instead\n",
    "\n",
    "    return F.log_softmax(final_layer, dim=1)\n",
    "\n",
    "\n",
    "  def start_and_end_from_center(self, width, i):\n",
    "    start = i - np.ceil(width)\n",
    "    end = i + np.floor(width) + 1\n",
    "    return (int(start), int(end))\n",
    "\n",
    "\n",
    "\n",
    "  def convolve(self, Kernel, Data, i):\n",
    "\n",
    "    # i is center index so we take equal on either side\n",
    "    T = Kernel.shape[1]\n",
    "    each_side = (T - 1) / 2\n",
    "\n",
    "    # Moves it backwards 1 if kernel is even\n",
    "    start, end = self.start_and_end_from_center(each_side, i)\n",
    "\n",
    "    adj_data = Data[:, start:end, :]\n",
    "\n",
    "    # So now K is 5 x L and adj_data is N x L X 5\n",
    "\n",
    "\n",
    "    N = adj_data.shape[0]\n",
    "\n",
    "    m = torch.bmm(Kernel.repeat(N,1,1), adj_data) # bij, bjk -> bik Slightly faster than einsum\n",
    "\n",
    "    # m = torch.einsum(\"ij, bjk -> bik\", Kernel, adj_data) #identical batch matmul\n",
    "\n",
    "    diag = torch.einsum(\"bii->bi\", m)\n",
    "    # diags = []\n",
    "    # for res in m:\n",
    "    #   diags.append(torch.diag(res))\n",
    "\n",
    "\n",
    "    return diag\n",
    "\n",
    "  def get_residual_vectors(self, narrow, mid, wide):\n",
    "    # Should be N x 8m x Features\n",
    "    # print(narrow.shape, mid.shape, wide.shape)\n",
    "    N = wide.shape[0]\n",
    "    L = wide.shape[1]\n",
    "\n",
    "    apply_avg = lambda matrix: torch.transpose(self.__avgpool__(torch.transpose(matrix, 1,2)), 1,2)\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    if L % 8:\n",
    "      raise Exception(\"Something went wrong, convs are not mult of 8\")\n",
    "    for i in range(L // 8):\n",
    "\n",
    "      n = torch.autograd.Variable(narrow[:, 8*i:8*(i+1), :])\n",
    "      m = torch.autograd.Variable(mid[:, 8*i:8*(i+1), :])\n",
    "      w = torch.autograd.Variable(wide[:, 8*i:8*(i+1), :])\n",
    "\n",
    "      n = apply_avg(n)\n",
    "      m = apply_avg(m)\n",
    "      w = apply_avg(w)\n",
    "\n",
    "      n = n.reshape(N, 2*self.features)\n",
    "      m = m.reshape(N, 2*self.features)\n",
    "      w = w.reshape(N, 2*self.features)\n",
    "      # gives N x 2 * Features\n",
    "\n",
    "      vectors.append(torch.autograd.Variable(torch.cat((n,m,w), dim=1))) # N x 6 * features\n",
    "\n",
    "    return vectors\n",
    "\n",
    "class lstm(nn.Module):\n",
    "  def __init__(self, into, hidden, out):\n",
    "    super().__init__()\n",
    "\n",
    "    self.rl = nn.functional.sigmoid\n",
    "    self.ls = nn.LSTM(into, hidden, batch_first=True)\n",
    "    self.to_out = nn.Linear(hidden,out)\n",
    "    self.prob = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    _, (f, __) = self.ls(x)\n",
    "\n",
    "    return self.to_out(f.squeeze())\n",
    "\n",
    "class lstm2(nn.Module):\n",
    "  def __init__(self, into, hidden, out):\n",
    "    super().__init__()\n",
    "\n",
    "    self.rl = nn.functional.sigmoid\n",
    "    self.ls = nn.LSTM(into, hidden, batch_first=True, num_layers=2)\n",
    "    self.to_out = nn.Linear(hidden,out)\n",
    "    self.prob = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    _, (f, __) = self.ls(x)\n",
    "\n",
    "    return self.to_out(f[1, :].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1698123021980,
     "user": {
      "displayName": "Matteo",
      "userId": "07662895649750931658"
     },
     "user_tz": 420
    },
    "id": "QCaXFn0b4T4E",
    "outputId": "046486bb-7ddf-4d47-ff86-8c8da5e3c5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82997\n"
     ]
    }
   ],
   "source": [
    "model = FluxAnomalyPredictionTF(20, 0.15, bn=False, features=3, residual=0).to(device)\n",
    "\n",
    "model_pars = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_pars])\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1698123023002,
     "user": {
      "displayName": "Matteo",
      "userId": "07662895649750931658"
     },
     "user_tz": 420
    },
    "id": "BOJnl8dTJEoi",
    "outputId": "4aa465eb-c350-44e1-9ba4-8fb3b276d6dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 498, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = DataLoader(data_train, batch_size=len(data_train), shuffle=True, collate_fn=padded_collate)\n",
    "valid = DataLoader(data_valid, batch_size=len(data_valid), shuffle=True, collate_fn=padded_collate)\n",
    "next(iter(train))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX8cN71g14oT"
   },
   "source": [
    "# Training Loop (Single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DmGh2Yd4kclP"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "blue"
         },
         "mode": "lines",
         "name": "Training Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1.825012198107797,
          1.6573755319397372,
          1.5525252912799614,
          1.4829732795739923,
          1.445029666989128,
          1.4605569765993138,
          1.426487445865845,
          1.440834738157993,
          1.4141693080864557,
          1.4128483489597636,
          1.4132478754937723,
          1.4037188715234779,
          1.4069796339973868,
          1.4016273117422913,
          1.409536887631208,
          1.4100147621485115,
          1.399325356740897,
          1.3937848963237107,
          1.3942564944322398,
          1.3987380653940882,
          1.3974343289932483,
          1.4054614252029718,
          1.3981814030093558,
          1.3963753752063952,
          1.3946710519234462,
          1.393099420592875,
          1.389634676980933,
          1.3898694301128272,
          1.3921354974775866,
          1.3848202417984496,
          1.3859411915519255,
          1.3949849305880728,
          1.3851712897296258,
          1.3912478014170413,
          1.393075882983791,
          1.3890167755369878,
          1.3885129930811309,
          1.3904800319756478,
          1.3853747561794951,
          1.3869148657180779,
          1.3913697494517783,
          1.38842653904524,
          1.3842975405554971,
          1.3838906182655293,
          1.388171339515435,
          1.3902597751710848,
          1.385767984097401,
          1.3892387699539823,
          1.3859576925593338,
          1.3853910075732463,
          1.3860245454361786,
          1.389105275050862,
          1.389255996291783,
          1.3858706720832419,
          1.3856675374803686,
          1.3854890449233102,
          1.3860781191282983,
          1.3871295540454909,
          1.387607831790125,
          1.3883971375110065,
          1.390522510076571,
          1.3859937993314866,
          1.3842011644681993,
          1.3851002392739844,
          1.3835240671025502,
          1.3835171637433687,
          1.3848872372984202,
          1.385787091017719,
          1.3844903770777663,
          1.3826062255385188,
          1.3878876034136929,
          1.3812215249290254,
          1.381802785299719,
          1.3797698143573836,
          1.3762601631883171,
          1.377587626232233,
          1.3688663638508765,
          1.3745836743071005,
          1.3736620047086088,
          1.3730581507726,
          1.3691668061096611,
          1.3670325547919835,
          1.3679615295573406,
          1.3635207200709252,
          1.3513801281411142,
          1.3615136422880838,
          1.3440422384122646,
          1.3477670408547675,
          1.3396484097787578,
          1.3300365071857745,
          1.3348878993111337,
          1.3300372685004258,
          1.3284337291782795,
          1.3344371160320534,
          1.3101364998699974,
          1.3058908239208789,
          1.306189729882007,
          1.3035097346869393,
          1.2803427128871114,
          1.278080634046138,
          1.2937927388928705,
          1.2645965938713482,
          1.2521621394014129,
          1.2589278214690638,
          1.233504673159546,
          1.2422612165013642,
          1.222264720316197,
          1.222574641115817,
          1.2132401845928171,
          1.2312539406612109,
          1.2210478388679604,
          1.1742800402992695,
          1.202571876175196,
          1.2122517994209963,
          1.1929101467313665,
          1.1831820527205679,
          1.1914353768306312,
          1.1862952058097886,
          1.1884407416341523,
          1.1909442284580571,
          1.1815796970003112,
          1.1730503985327871,
          1.1734569550376672,
          1.1716715943720757,
          1.1564023771805603,
          1.1712482704376685,
          1.1628044527646353,
          1.1803407894077325,
          1.1490508845472893,
          1.1337738306995253,
          1.1206742767581257,
          1.1455161031018093,
          1.1364472464167648,
          1.1312659681436856,
          1.14210613456741,
          1.1318038609920233,
          1.1178774926341661,
          1.1248890798632405,
          1.1375191950314187,
          1.1344909293552352,
          1.0833148097171217,
          1.1288646863588858,
          1.0927340411485806,
          1.0947366950462802,
          1.1174644743156215,
          1.0731787784233195,
          1.0790415776848272,
          1.124246653753071,
          1.0751280673978045,
          1.0792408020634512,
          1.0686097175160816,
          1.0674398054467413,
          1.0577139258058954,
          1.06871123832591,
          1.085361502157935,
          1.0462162229068335,
          1.0530069834820461,
          1.0376010932408133,
          1.0736990638467148,
          1.0830635116769392,
          1.064941527440023,
          1.0479456775968508,
          1.0522441228082278,
          1.0493337438551353,
          1.0777077159587671,
          1.0567315717505459,
          1.0584946780161422,
          1.0880171528053182,
          1.0524728445449665,
          1.1006403304703412,
          1.1055997912775777,
          1.123610724403846,
          1.068420275741816,
          1.0341479037604715,
          1.0808947700726,
          1.0695073161770268,
          1.0449818036122265,
          1.091014505855516,
          1.0121827581142588,
          1.0497484889312325,
          1.068353839211397,
          1.0548014835010748,
          1.0476977347187093,
          1.0522578583357385,
          1.0405954007697382,
          1.056605429998804,
          1.0506441469216616,
          1.0518737218874061,
          1.047199768745015,
          1.0429880895076413,
          1.0260594439248922,
          1.0523622228815808,
          1.0350974561168624,
          1.0578003002755998,
          1.0311068410649136,
          1.0378304215463459,
          1.0196419808039967,
          1.0509247776868575,
          1.011989575661207,
          1.0146973035549922,
          1.0079405230592822,
          1.0449561436963237,
          0.9793474981343323,
          0.9701981324064787,
          0.9960748344057744,
          1.0048715735860174,
          1.0035833770737557,
          1.0196702050797841,
          1.0321910504164522,
          0.9987720507137264,
          0.9991926768715041,
          0.9997717555490121,
          0.9722791648799822,
          0.9417959167135734,
          0.9755869055036623,
          0.9643091681484951,
          0.9791678688323124,
          0.9569415146869139,
          0.9987604690930716,
          0.9730623021959646,
          0.9643962481906788,
          0.9476654432582345,
          0.9948358283411516,
          0.9641609955424756,
          0.9726288900349906,
          0.9702892936479498,
          0.9717897140365079,
          0.9546167100931482,
          0.9935336144335705,
          0.9809622507528263,
          0.9798200067785745,
          0.9446680772288382,
          0.961009063567935,
          0.944315794526203,
          0.9458296718413298,
          0.9843822608886617,
          0.9453765220093806,
          0.9580999651176639,
          0.9568223174452656,
          0.9797888938316052,
          0.9830775170828948,
          0.9764208138716177,
          0.9751966581833607,
          0.9587739149542733,
          0.9625381457413554,
          0.971877125479899,
          0.9756286778395402,
          0.9769207317357442,
          0.9923156831840065,
          0.9682738010570634,
          0.9537725149068857,
          1.0184271601305153,
          0.9576103593992055,
          0.9592325440264965,
          0.9705458246749642,
          0.9708688494125227,
          0.9537136230957654,
          0.9741635823865697,
          0.9215263957562392,
          0.9633392158961694,
          0.9537955930686036,
          0.9348122499853534,
          0.9630635932894376,
          0.9667038343023797,
          0.9747931963351849,
          0.9758634484827806,
          0.9261880418221953,
          0.958342197143709,
          0.9731828689933233,
          0.9630127668991477,
          0.9774957348357518,
          0.9573031565480922,
          0.9253684375335399,
          0.960532891388309,
          0.9389204684805444,
          0.9307445709921774,
          0.9477808943939867,
          0.9386913693332761,
          0.9282413110112206,
          0.9238286184940053,
          0.9246859411200937,
          0.9554286146218595,
          0.9342958817305674,
          0.9188288343966061,
          0.9274060253012597,
          0.9504053661357503,
          0.919355549253405,
          0.9292927173567591,
          0.9194436786885525,
          0.9038586714052942,
          0.931735526458988,
          0.9646991427683153,
          0.9008659372806852,
          0.9122778354064998,
          0.9353673055972652,
          0.9097670976457567,
          0.9021135312612029,
          0.9136885632803478,
          0.925872612198832,
          0.9317096914322256,
          0.9186505809660713,
          0.9124619732628239,
          0.957571310286551,
          0.9544681856038945,
          0.9028889957863135,
          0.9137349378224162,
          0.940029965370896,
          0.9317993145025715,
          0.921986690110408,
          0.9232369080324996,
          0.9318655056478194,
          0.9532085281360471,
          0.9353283680157612,
          0.983649622665624,
          0.8735346299709941,
          0.8832924544340364,
          0.9286461063659076,
          0.8903247978378286,
          0.9071324885645353,
          0.9114565883818472,
          0.9155245875713796,
          0.8923317547893831,
          0.8849025782776526,
          0.8892747806747751,
          0.8974798828765544,
          0.9279256137451316,
          0.9210002288874938,
          0.9182900327529777,
          0.9270894075474937,
          0.9179247192044144,
          0.9289466320331282,
          0.8794978413943814,
          0.8690725966108637,
          0.9017223912791251,
          0.889185106713813,
          0.9167000670809482,
          0.8843353823113854,
          0.8749862743323684,
          0.8770374798087114,
          0.872615411778869,
          0.8873351949372271,
          0.9274753064484956,
          0.8800152127294361,
          0.9036271259846893,
          0.9014983340223542,
          0.8824893435906181,
          0.8518862162792691,
          0.8942736134787814,
          0.8651254979498674,
          0.8666258900802949,
          0.8896552960950447,
          0.8732275313035472,
          0.8767969245344059,
          0.904847852787173,
          0.8935899203749559,
          0.9009929621761329,
          0.9517654094322564,
          0.8849658829434259,
          0.891422391598239,
          0.828599225106188,
          0.8665454700317923,
          0.8745169615673566,
          0.9026723444301936,
          0.926327483114099,
          0.8962881426427148,
          0.9071400647412836,
          0.9077311009185849,
          0.9130667242201201,
          0.8895383027596158,
          0.8876443815096512,
          0.9101536389634443,
          0.8806210725622456,
          0.8996282831894024,
          0.8887577976025786,
          0.8694618451198379,
          0.8975856532783699,
          0.878462235338262,
          0.8940360048430106,
          0.8993119078422022,
          0.8852631234605336,
          0.882871370381999,
          0.8398914534205865,
          0.8938770602673402,
          0.8206939572231176,
          0.8919886470039821,
          0.8853674964427368,
          0.9371930416029459,
          0.8658580768045093,
          0.9002177495364274,
          0.8853425995604678,
          0.9048158800831588,
          0.8828267067825493,
          0.8452869588228727,
          0.9027253139296364,
          0.880946585669749,
          0.8508434057402984,
          0.8600370363557734,
          0.8563119373104234,
          0.8763031329629885,
          0.8820220505622712,
          0.9007487865979177,
          0.8758858573009507,
          0.8987033488619586,
          0.881604533550304,
          0.8593704276124253,
          0.9009900435056373,
          0.8971299052708704,
          0.9133253876741062,
          0.8729884530539054,
          0.8281525247070055,
          0.8118876460402611,
          0.8905506422121721,
          0.8439471044706706,
          0.8797893320150337,
          0.8736482297608924,
          0.8501448121867502,
          0.8526385052577721,
          0.855962078379928,
          0.8639981351296104,
          0.8867703210455455,
          0.8769456796801071,
          0.8822511627215477,
          0.8664831089940511,
          0.8925658063669771,
          0.8958016580596372,
          0.8670939184360311,
          0.9079037136547616,
          0.8734516496744499,
          0.8639172776609015,
          0.8436433657891506,
          0.8722787149427133,
          0.8176049532859009,
          0.8854834583516704,
          0.8676209602697894,
          0.9016084120415496,
          0.8420579231436478,
          0.8779445145263689,
          0.8882675761284574,
          0.857026847488332,
          0.8477218179745136,
          0.8542472447140578,
          0.8406266781734802,
          0.8460933538611844,
          0.8665254670535916,
          0.858875030071267,
          0.8860822786113101,
          0.8720052834375454,
          0.8568856149291668,
          0.8908085082297739,
          0.8911237994264404,
          0.9042076480967597,
          0.8682704374049657,
          0.9052598255245233,
          0.8427438261743486,
          0.8616773066296013,
          0.8569101136405275,
          0.8737068568634799,
          0.8653305824446645,
          0.8526960578992859,
          0.8509157028802866,
          0.8526527159624884,
          0.8480827690790861,
          0.8843142973897548,
          0.8414296757455818,
          0.894683831160282,
          0.8325660127472576,
          0.8839469956277636,
          0.8616108936753463,
          0.8077111389486095,
          0.8385358170388083,
          0.8583893097110008,
          0.8363208007022988,
          0.8388233150626522,
          0.8341443853102205,
          0.8861648484163154,
          0.8714661958457174,
          0.8751430472998006,
          0.9154423371617554,
          0.8652527254381812,
          0.9121835454021984,
          0.8688141731479238,
          0.8669651009749386,
          0.8566165078069475,
          0.9076801771204542,
          0.8923484907929659,
          0.8934848672439633,
          0.8458480926689531,
          0.840332310349642,
          0.8949188663097412,
          0.8414150535604187,
          0.8281473659498544,
          0.8363104023570308,
          0.83221837598322,
          0.8432985697225556,
          0.8409588021523919,
          0.8389431325707942,
          0.860333055406001,
          0.8779595892312495,
          0.8423041781079778,
          0.8903766844027491
         ]
        },
        {
         "line": {
          "color": "orange"
         },
         "mode": "lines",
         "name": "Validation Loss",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          1.554329407352787,
          1.4883683753632913,
          1.443509826321007,
          1.4162623874037001,
          1.3988665332606507,
          1.3942744409995405,
          1.3922356199844268,
          1.3904702924907515,
          1.3899677584650425,
          1.3936665380883373,
          1.395143822389006,
          1.3959207420660713,
          1.3966689814158364,
          1.3973589004059033,
          1.3982751041662473,
          1.3991719201193433,
          1.3999027702717335,
          1.4001221300923925,
          1.40024643932185,
          1.4001124217516285,
          1.3994550282213236,
          1.3987611496268013,
          1.3976189799427974,
          1.396311824160187,
          1.3952361078843174,
          1.3940288520702964,
          1.3928305905074378,
          1.391707121997505,
          1.3907216680495245,
          1.389918922902882,
          1.3888793436010605,
          1.3884690184275168,
          1.3884712702141477,
          1.388480572734478,
          1.388534843395303,
          1.3886184710230631,
          1.3887767022771884,
          1.388887747134693,
          1.388996749944211,
          1.3890778906701373,
          1.389149812537469,
          1.3892165598629846,
          1.3889447663482433,
          1.388489676801026,
          1.3880306804786,
          1.387595910851551,
          1.3873920556151085,
          1.3871721075053547,
          1.387057488731054,
          1.3869931905093962,
          1.3868687642725481,
          1.386775625315191,
          1.3868462988775299,
          1.3869820330665599,
          1.3871102905724022,
          1.3872918264783236,
          1.3877244149219046,
          1.3884921849619118,
          1.3892469927317432,
          1.3898336169797354,
          1.3904080549753342,
          1.3909210359067323,
          1.3913636978243964,
          1.3917390963329914,
          1.3919005852590025,
          1.3919225137159714,
          1.3922221850850267,
          1.392629091505064,
          1.392962500468955,
          1.393482910246883,
          1.393950315787934,
          1.3946992420600282,
          1.3957833488166729,
          1.3973330143183478,
          1.3994980465266416,
          1.4021088915363362,
          1.4054239045541193,
          1.4106355392883525,
          1.4132027728279566,
          1.4153734063263828,
          1.4175588217002255,
          1.4191257869239522,
          1.4201329117425134,
          1.4203721200298685,
          1.421997798414993,
          1.4249461768587635,
          1.4295787328280458,
          1.4356861275804356,
          1.4394668690950365,
          1.4405502320294892,
          1.4523008021875743,
          1.462232111789367,
          1.4710595506716064,
          1.4795430217564007,
          1.4876660097877565,
          1.487600194393887,
          1.4570204409140615,
          1.4242560270926121,
          1.4175209797748811,
          1.344702459334097,
          1.3040838534256405,
          1.3699186002340997,
          1.3952765184036313,
          1.3875754855530864,
          1.3655249722371068,
          1.382171822303464,
          1.3505386597476805,
          1.3348791687320887,
          1.2430913771349845,
          1.1892345205575907,
          1.1752879018030933,
          1.1768262727195973,
          1.1666739312182897,
          1.134121192352333,
          1.1203305106565784,
          1.111488534929914,
          1.110022032190888,
          1.112859898849353,
          1.1242996395557834,
          1.127939500539448,
          1.1277429090045417,
          1.1263766277868372,
          1.1222527387682144,
          1.1266220358709556,
          1.1359959990243786,
          1.1453961408924156,
          1.161686307124755,
          1.1720992898197438,
          1.1945201815837008,
          1.2079325695284766,
          1.202101448184752,
          1.165820723455207,
          1.1559766804913476,
          1.161430290947712,
          1.1822351337163184,
          1.2568235605696783,
          1.3798383395186795,
          1.2033704117242712,
          1.0967949487869342,
          1.03698260691774,
          1.0227406165482076,
          1.055044514581149,
          1.1258492069580706,
          1.178640951715809,
          1.1000208811735495,
          1.028378762932358,
          1.000410383022174,
          0.9886809325939313,
          0.9847290547670507,
          0.9853502982860062,
          1.010666332769962,
          1.0306932944417047,
          1.032071977151376,
          1.0268426074873969,
          1.0030852616874995,
          0.979558426092209,
          0.9575252241189486,
          0.9422939686476806,
          0.9428082153587786,
          0.9713571743862337,
          0.9970119930504774,
          1.0093062091997997,
          0.9928982683252144,
          0.9591586189807414,
          0.9207793893447377,
          0.9152515664837391,
          0.9156377079384089,
          0.9158441240370195,
          0.9156637474305954,
          0.9153900916201202,
          0.9157417750082452,
          0.9215179166044042,
          0.9438780495264859,
          0.9694878193371771,
          0.9932003938761925,
          1.015523883696611,
          1.0301846433864952,
          1.0476391085827934,
          1.0234619813411303,
          0.9998565075156595,
          0.9824457058892065,
          0.9683135870067751,
          0.9704374323692875,
          0.9708531100010864,
          0.9754908448585123,
          0.9700859584331898,
          0.9663815214477348,
          0.9661972454296516,
          0.9603415794098901,
          0.9511830828362479,
          0.9365816633453617,
          0.9312847191954722,
          0.928644623090737,
          0.9286633678650422,
          0.9286950692628174,
          0.9271106976338955,
          0.9222052676596709,
          0.918815604496439,
          0.9154894085028649,
          0.9118894541186634,
          0.9098535623073328,
          0.9086107063439872,
          0.9068960137027169,
          0.905441589916812,
          0.9036849456126587,
          0.9016522699216806,
          0.9004383419332558,
          0.8996817508468866,
          0.8988188051205176,
          0.8982544021702425,
          0.8979469533877552,
          0.8984540848414347,
          0.8992574583328693,
          0.8997200649210019,
          0.899452403040156,
          0.8983115542478884,
          0.8965221986767761,
          0.8950098079610947,
          0.8955103839078119,
          0.8951176661820491,
          0.893808619309343,
          0.8920343432759988,
          0.8927511866530141,
          0.8941714900609029,
          0.8934945614980325,
          0.8923251030298034,
          0.8916847581530276,
          0.8912450031268593,
          0.8904737267900552,
          0.8899202862145165,
          0.890461162799585,
          0.8917250257357194,
          0.8932368264941339,
          0.8928312852571466,
          0.890453065382048,
          0.888236389807305,
          0.8871203253070158,
          0.8876138174138837,
          0.8884279495250157,
          0.8884645973998286,
          0.88714116500954,
          0.8861138663625535,
          0.8850041018165138,
          0.8841893826484539,
          0.884195027322705,
          0.885498122626279,
          0.8842858695754284,
          0.8834753016553233,
          0.8821407967762025,
          0.8805713352560424,
          0.8791246301261068,
          0.8788572954178583,
          0.8786769742841485,
          0.8733778703414107,
          0.8661255716622565,
          0.8635066536466804,
          0.8614959991505566,
          0.8576403914743732,
          0.8548677204497106,
          0.853724775409434,
          0.85133841897657,
          0.8518880208531039,
          0.8524530746922944,
          0.8532997057913506,
          0.8540621722136114,
          0.8562157532837015,
          0.8569802415290407,
          0.8562406250165703,
          0.8574202144806314,
          0.8575071751222372,
          0.8575678187222606,
          0.8572289305040061,
          0.8557609999980418,
          0.8543562603099822,
          0.8531732437786279,
          0.8537663840461933,
          0.8595445284918212,
          0.8678647393412016,
          0.8706742977044898,
          0.8670572727817237,
          0.8501340759815411,
          0.8387338736409796,
          0.8331218495540019,
          0.8305661222943952,
          0.8306809421645237,
          0.8265349627877129,
          0.822062843812745,
          0.8181913609367849,
          0.8155098343464737,
          0.813818311118435,
          0.8114284079774405,
          0.8104678291761784,
          0.8098420693364755,
          0.8086528897813959,
          0.8089417637786652,
          0.8098198872255061,
          0.8099943597560295,
          0.8078948468390225,
          0.8056456295976901,
          0.8035943924488121,
          0.8017160374791857,
          0.8013674609503999,
          0.8017134965424324,
          0.801911887835223,
          0.8020595837546908,
          0.8019384447579571,
          0.8013794727252126,
          0.8009297785439323,
          0.8012519204752816,
          0.8037952826510204,
          0.8068604855694937,
          0.8095595208734645,
          0.8096902399357416,
          0.8085720863095526,
          0.8075778706006514,
          0.8071786915928915,
          0.806808373442912,
          0.8065408071961903,
          0.8064263415513088,
          0.8054761122360127,
          0.8041030154145475,
          0.8019336251979645,
          0.8010855240213387,
          0.8007690314764272,
          0.8001921152314359,
          0.7996012488001701,
          0.7989222117264627,
          0.7993958436916074,
          0.8001476052655185,
          0.8008603691424416,
          0.8009712941135536,
          0.7997466910838918,
          0.795998269354666,
          0.7893970378546461,
          0.7868143664855163,
          0.7852896227147902,
          0.7847546815993863,
          0.7868289746749081,
          0.788093571638551,
          0.7880068186244028,
          0.7869041214093814,
          0.7846593913534111,
          0.7825941323571369,
          0.7816607363695415,
          0.781958098564569,
          0.7819617202639476,
          0.7812772263180697,
          0.7815673067982777,
          0.7826432293539383,
          0.7846285384501406,
          0.7870000053872656,
          0.7880868390003274,
          0.7899505385454465,
          0.7895115854969298,
          0.7864309809274804,
          0.783511202348306,
          0.7814010342294001,
          0.779838371343884,
          0.7797055471930955,
          0.7805525377121683,
          0.783608484896122,
          0.7877837525337273,
          0.7877126847205069,
          0.787046880130065,
          0.7854728303868895,
          0.7845134289801958,
          0.7845104404191329,
          0.7844759901558334,
          0.7841004441096656,
          0.7833321424098895,
          0.78232068269013,
          0.7809398223754593,
          0.7800491569271589,
          0.7791236385906247,
          0.7783190078888506,
          0.7816168721471818,
          0.7844459189620627,
          0.7852629535460528,
          0.7846597654397461,
          0.7837943475370706,
          0.7816119771150891,
          0.7790085258704685,
          0.7769878808438916,
          0.7760878531890959,
          0.7752387048161018,
          0.775672463532097,
          0.7758321592740582,
          0.7764303686924497,
          0.777882915843966,
          0.7788204592131419,
          0.7803189750549923,
          0.7811541963664248,
          0.7815831830885903,
          0.7815496849712794,
          0.7808217267815315,
          0.7791867165359049,
          0.7775607295568431,
          0.7770309536759092,
          0.7766091811785648,
          0.7755152495667628,
          0.7747489581757604,
          0.7687319619711941,
          0.7688141816616522,
          0.7692668038146185,
          0.7703578151173925,
          0.771657771015453,
          0.7729990284635032,
          0.7745968156363184,
          0.7770332527727458,
          0.7769770566785038,
          0.7782943958147932,
          0.7775395537777395,
          0.7761354158725692,
          0.7744501373429896,
          0.769701886345607,
          0.7681514308222072,
          0.7674994783556724,
          0.7667985857072308,
          0.769042099536637,
          0.7705740208102313,
          0.7701153138089095,
          0.7692974975168143,
          0.7687959062179575,
          0.768677994307204,
          0.7678095767207133,
          0.7651975474932778,
          0.7718971015060513,
          0.7732437726961964,
          0.7746555531852367,
          0.7748435467035026,
          0.7743178757689112,
          0.7729281606324654,
          0.7667676420742957,
          0.7659883160615316,
          0.7644904287327984,
          0.763210520683487,
          0.7632302482075796,
          0.7695530935075978,
          0.7705402730692477,
          0.7703459740327392,
          0.7691867797883486,
          0.7695430863767712,
          0.7698381671207631,
          0.7707715081784868,
          0.770590067547738,
          0.7698763672190516,
          0.7695315620039138,
          0.7688679950334601,
          0.7682688043712406,
          0.7691505598684495,
          0.7696714364383102,
          0.7699079692637184,
          0.7698378773842928,
          0.7701003705472079,
          0.7719065920772316,
          0.7766336580337496,
          0.7814622149384568,
          0.7839763373774861,
          0.7818068494336169,
          0.7774068017035645,
          0.7729610754104829,
          0.7709012772420293,
          0.7694787719938921,
          0.7690978769656436,
          0.766680807052846,
          0.7645670113754294,
          0.7629266633579047,
          0.7613075157642245,
          0.7603734331618602,
          0.7603618536058697,
          0.7607027266156869,
          0.7608174990501548,
          0.7555388424214149,
          0.7544223816847264,
          0.7552526387427787,
          0.7556019115554743,
          0.7554690451073243,
          0.7578808582759665,
          0.7657629139264696,
          0.7642217872324237,
          0.763107063063301,
          0.7610110376198717,
          0.7582397027948092,
          0.7613168707902559,
          0.7623807337716744,
          0.7621974830444547,
          0.765730934083367,
          0.763954020709023,
          0.7642865439821981,
          0.7665533984670757,
          0.7686225749226382,
          0.7696041183268044,
          0.7699815001868743,
          0.7697363143814906,
          0.7701304914096562,
          0.7656855946289186,
          0.7641871160552176,
          0.7643476558801093,
          0.7648434522972273,
          0.7668751816634091
         ]
        },
        {
         "line": {
          "color": "gray"
         },
         "mode": "lines",
         "name": "Null Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.09090908264462885,
          0.1818181652892577,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.09090908264462885,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9999999090909174,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.9999999090909174,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598,
          0.9090908264462886,
          0.8181817438016598,
          0.8181817438016598,
          0.8181817438016598
         ]
        },
        {
         "line": {
          "color": "yellow"
         },
         "mode": "lines",
         "name": "Nova Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001,
          0.99999990000001
         ]
        },
        {
         "line": {
          "color": "green"
         },
         "mode": "lines",
         "name": "Pulsator Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.1428570408163994,
          0.21428556122459913,
          0.21428556122459913,
          0.1428570408163994,
          0.1428570408163994,
          0.2857140816327988,
          0.21428556122459913,
          0.2857140816327988,
          0.1428570408163994,
          0,
          0,
          0.0714285204081997,
          0.2857140816327988,
          0.7857137244901968,
          0.9285707653065962,
          0.42857112244919826,
          0,
          0,
          0,
          0.35714260204099857,
          0.35714260204099857,
          0.35714260204099857,
          0.5714281632655976,
          0.49999964285739795,
          0.7142852040819971,
          0.5714281632655976,
          0.5714281632655976,
          0.5714281632655976,
          0.5714281632655976,
          0.7142852040819971,
          0.7142852040819971,
          0.8571422448983965,
          0.9285707653065962,
          0.7142852040819971,
          0.6428566836737974,
          0.5714281632655976,
          0.6428566836737974,
          0.7142852040819971,
          0.9285707653065962,
          0.9999992857147959,
          0,
          0,
          0,
          0,
          0.1428570408163994,
          0.42857112244919826,
          0.6428566836737974,
          0.7142852040819971,
          0.6428566836737974,
          0.6428566836737974,
          0.5714281632655976,
          0.5714281632655976,
          0.6428566836737974,
          0.5714281632655976,
          0.42857112244919826,
          0.49999964285739795,
          0.5714281632655976,
          0.5714281632655976,
          0.5714281632655976,
          0.7857137244901968,
          0.7857137244901968,
          0.5714281632655976,
          0.35714260204099857,
          0.21428556122459913,
          0.21428556122459913,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.1428570408163994,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.1428570408163994,
          0.21428556122459913,
          0.2857140816327988,
          0.35714260204099857,
          0.2857140816327988,
          0.21428556122459913,
          0.0714285204081997,
          0.21428556122459913,
          0.21428556122459913,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0.2857140816327988,
          0.2857140816327988,
          0.35714260204099857,
          0.2857140816327988,
          0.2857140816327988,
          0.1428570408163994,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0,
          0.0714285204081997,
          0.1428570408163994,
          0.35714260204099857,
          0.21428556122459913,
          0.2857140816327988,
          0.21428556122459913,
          0.21428556122459913,
          0.0714285204081997,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.35714260204099857,
          0.5714281632655976,
          0.6428566836737974,
          0.6428566836737974,
          0.7142852040819971,
          0.7857137244901968,
          0.7857137244901968,
          0.7857137244901968,
          0.8571422448983965,
          0.8571422448983965,
          0.7857137244901968,
          0.7857137244901968,
          0.7142852040819971,
          0.5714281632655976,
          0.35714260204099857,
          0.2857140816327988,
          0.1428570408163994,
          0.0714285204081997,
          0.1428570408163994,
          0.21428556122459913,
          0.1428570408163994,
          0.1428570408163994,
          0.1428570408163994,
          0.2857140816327988,
          0.42857112244919826,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.42857112244919826,
          0.35714260204099857,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.21428556122459913,
          0.1428570408163994,
          0.21428556122459913,
          0.21428556122459913,
          0.5714281632655976,
          0.6428566836737974,
          0.6428566836737974,
          0.6428566836737974,
          0.8571422448983965,
          0.8571422448983965,
          0.8571422448983965,
          0.8571422448983965,
          0.49999964285739795,
          0.1428570408163994,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0.1428570408163994,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0.0714285204081997,
          0.1428570408163994,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0.0714285204081997,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0.0714285204081997,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        },
        {
         "line": {
          "color": "purple"
         },
         "mode": "lines",
         "name": "Transit Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.15384603550304962,
          0.23076905325457442,
          0.30769207100609924,
          0.6153841420121985,
          0.6923071597637233,
          0.46153810650914884,
          0.46153810650914884,
          0.46153810650914884,
          0.5384611242606736,
          0.46153810650914884,
          0.5384611242606736,
          0.6153841420121985,
          0.7692301775152481,
          0.9230762130182977,
          0.9230762130182977,
          0.9230762130182977,
          0.9230762130182977,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.46153810650914884,
          0.38461508875762407,
          0.38461508875762407,
          0.30769207100609924,
          0.30769207100609924,
          0.38461508875762407,
          0.46153810650914884,
          0.5384611242606736,
          0.5384611242606736,
          0.6153841420121985,
          0.46153810650914884,
          0.46153810650914884,
          0.23076905325457442,
          0,
          0,
          0.6153841420121985,
          0.6153841420121985,
          0.6923071597637233,
          0.7692301775152481,
          0.7692301775152481,
          0.6923071597637233,
          0.7692301775152481,
          0.6923071597637233,
          0.5384611242606736,
          0.30769207100609924,
          0.23076905325457442,
          0.15384603550304962,
          0.15384603550304962,
          0.23076905325457442,
          0.46153810650914884,
          0.5384611242606736,
          0.23076905325457442,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9999992307698226,
          0.9230762130182977,
          0.6923071597637233,
          0.5384611242606736,
          0.30769207100609924,
          0.23076905325457442,
          0.23076905325457442,
          0.15384603550304962,
          0.15384603550304962,
          0.15384603550304962,
          0.23076905325457442,
          0.38461508875762407,
          0.46153810650914884,
          0.38461508875762407,
          0.38461508875762407,
          0.30769207100609924,
          0.38461508875762407,
          0.38461508875762407,
          0.38461508875762407,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.15384603550304962,
          0.23076905325457442,
          0.23076905325457442,
          0.46153810650914884,
          0.46153810650914884,
          0.46153810650914884,
          0.23076905325457442,
          0.15384603550304962,
          0.15384603550304962,
          0.15384603550304962,
          0.23076905325457442,
          0.23076905325457442,
          0.23076905325457442,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0.15384603550304962,
          0.15384603550304962,
          0.07692301775152481,
          0.07692301775152481,
          0.07692301775152481,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.07692301775152481,
          0.07692301775152481,
          0.23076905325457442,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.30769207100609924,
          0.07692301775152481,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0.07692301775152481,
          0.30769207100609924,
          0.5384611242606736,
          0.6153841420121985,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.6153841420121985,
          0.6153841420121985,
          0.5384611242606736,
          0.5384611242606736,
          0.38461508875762407,
          0.38461508875762407,
          0.38461508875762407,
          0.46153810650914884,
          0.5384611242606736,
          0.5384611242606736,
          0.46153810650914884,
          0.46153810650914884,
          0.38461508875762407,
          0.38461508875762407,
          0.46153810650914884,
          0.5384611242606736,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.5384611242606736,
          0.5384611242606736,
          0.46153810650914884,
          0.38461508875762407,
          0.38461508875762407,
          0.38461508875762407,
          0.5384611242606736,
          0.5384611242606736,
          0.6923071597637233,
          0.6923071597637233,
          0.7692301775152481,
          0.6923071597637233,
          0.6923071597637233,
          0.5384611242606736,
          0.5384611242606736,
          0.6153841420121985,
          0.6153841420121985,
          0.6923071597637233,
          0.6923071597637233,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.5384611242606736,
          0.5384611242606736,
          0.46153810650914884,
          0.46153810650914884,
          0.5384611242606736,
          0.6153841420121985,
          0.6923071597637233,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.6923071597637233,
          0.7692301775152481,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.7692301775152481,
          0.6923071597637233,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6153841420121985,
          0.6923071597637233,
          0.6923071597637233,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.6923071597637233,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481,
          0.7692301775152481
         ]
        },
        {
         "line": {
          "color": "red"
         },
         "mode": "lines",
         "name": "Total Validation Accuracy",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0.2708333333333333,
          0,
          0,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.22916666666666666,
          0.25,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.20833333333333334,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.22916666666666666,
          0.25,
          0.2708333333333333,
          0.2916666666666667,
          0.375,
          0.3958333333333333,
          0.3333333333333333,
          0.3333333333333333,
          0.3333333333333333,
          0.3541666666666667,
          0.3333333333333333,
          0.3541666666666667,
          0.375,
          0.4166666666666667,
          0.4583333333333333,
          0.4583333333333333,
          0.4583333333333333,
          0.4583333333333333,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.375,
          0.375,
          0.375,
          0.3333333333333333,
          0.3333333333333333,
          0.3958333333333333,
          0.3958333333333333,
          0.4375,
          0.3958333333333333,
          0.375,
          0.3541666666666667,
          0.3541666666666667,
          0.3541666666666667,
          0.4375,
          0.4791666666666667,
          0.5,
          0.375,
          0.3958333333333333,
          0.4166666666666667,
          0.5208333333333334,
          0.5,
          0.5208333333333334,
          0.5625,
          0.5,
          0.5,
          0.4375,
          0.4166666666666667,
          0.4166666666666667,
          0.4375,
          0.5416666666666666,
          0.5625,
          0.5208333333333334,
          0.4791666666666667,
          0.4166666666666667,
          0.3958333333333333,
          0.375,
          0.3958333333333333,
          0.4166666666666667,
          0.4791666666666667,
          0.5,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.5,
          0.5208333333333334,
          0.5416666666666666,
          0.5,
          0.4583333333333333,
          0.4583333333333333,
          0.4166666666666667,
          0.4166666666666667,
          0.4375,
          0.4375,
          0.4375,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.4583333333333333,
          0.5416666666666666,
          0.5416666666666666,
          0.4791666666666667,
          0.5416666666666666,
          0.5,
          0.5,
          0.4583333333333333,
          0.4583333333333333,
          0.4583333333333333,
          0.5,
          0.5,
          0.5,
          0.4791666666666667,
          0.4583333333333333,
          0.4583333333333333,
          0.4583333333333333,
          0.4791666666666667,
          0.5,
          0.5208333333333334,
          0.5416666666666666,
          0.5208333333333334,
          0.5,
          0.4583333333333333,
          0.5,
          0.5,
          0.4583333333333333,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4583333333333333,
          0.5208333333333334,
          0.5208333333333334,
          0.5416666666666666,
          0.5208333333333334,
          0.5208333333333334,
          0.4791666666666667,
          0.4583333333333333,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4375,
          0.4583333333333333,
          0.4375,
          0.4583333333333333,
          0.4791666666666667,
          0.5625,
          0.5208333333333334,
          0.5416666666666666,
          0.5208333333333334,
          0.5416666666666666,
          0.5208333333333334,
          0.5208333333333334,
          0.5625,
          0.5625,
          0.5625,
          0.5,
          0.4791666666666667,
          0.4791666666666667,
          0.4791666666666667,
          0.5,
          0.5,
          0.5,
          0.5416666666666666,
          0.6041666666666666,
          0.625,
          0.625,
          0.6458333333333334,
          0.6666666666666666,
          0.6666666666666666,
          0.6666666666666666,
          0.6875,
          0.6875,
          0.6666666666666666,
          0.6666666666666666,
          0.6458333333333334,
          0.6041666666666666,
          0.5625,
          0.5416666666666666,
          0.5,
          0.4791666666666667,
          0.5,
          0.5208333333333334,
          0.5,
          0.5208333333333334,
          0.5208333333333334,
          0.5416666666666666,
          0.5833333333333334,
          0.6458333333333334,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.6458333333333334,
          0.5833333333333334,
          0.6041666666666666,
          0.5833333333333334,
          0.5833333333333334,
          0.5625,
          0.5625,
          0.5625,
          0.5625,
          0.5625,
          0.5625,
          0.5625,
          0.5625,
          0.5416666666666666,
          0.5625,
          0.5833333333333334,
          0.625,
          0.625,
          0.625,
          0.6041666666666666,
          0.6666666666666666,
          0.6666666666666666,
          0.6666666666666666,
          0.6666666666666666,
          0.5833333333333334,
          0.5416666666666666,
          0.5625,
          0.5625,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5625,
          0.5625,
          0.5416666666666666,
          0.5625,
          0.5416666666666666,
          0.5625,
          0.5833333333333334,
          0.5833333333333334,
          0.5625,
          0.5625,
          0.5416666666666666,
          0.5416666666666666,
          0.5625,
          0.5625,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5625,
          0.5625,
          0.5625,
          0.5416666666666666,
          0.5625,
          0.5416666666666666,
          0.5625,
          0.5625,
          0.5833333333333334,
          0.5833333333333334,
          0.6041666666666666,
          0.5833333333333334,
          0.6041666666666666,
          0.5625,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.6041666666666666,
          0.6041666666666666,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5625,
          0.5833333333333334,
          0.5625,
          0.5625,
          0.5625,
          0.5625,
          0.5833333333333334,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.625,
          0.625,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.5833333333333334,
          0.6041666666666666,
          0.5833333333333334,
          0.5625,
          0.5625,
          0.5833333333333334,
          0.5833333333333334,
          0.5625,
          0.5625,
          0.5625,
          0.5625,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.625,
          0.5833333333333334,
          0.5833333333333334,
          0.6041666666666666,
          0.5833333333333334,
          0.5833333333333334,
          0.625,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.625,
          0.6041666666666666,
          0.6041666666666666,
          0.5833333333333334,
          0.5833333333333334,
          0.5625,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.625,
          0.6041666666666666,
          0.5833333333333334,
          0.6041666666666666,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.6041666666666666,
          0.5833333333333334,
          0.5625,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.5833333333333334,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.625,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.5833333333333334,
          0.625,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.5833333333333334,
          0.5833333333333334,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666,
          0.625,
          0.6041666666666666,
          0.6041666666666666,
          0.6041666666666666
         ]
        }
       ],
       "layout": {
        "legend": {
         "x": 1,
         "xanchor": "right",
         "y": 1,
         "yanchor": "top"
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training and Validation Loss Over Epochs: 499/500"
        },
        "xaxis": {
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "range": [
          0,
          500
         ],
         "showline": true,
         "tickfont": {
          "size": 12
         },
         "title": {
          "text": "Epochs"
         }
        },
        "yaxis": {
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "range": [
          0,
          2
         ],
         "showline": true,
         "tickfont": {
          "size": 12
         },
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  499 :  0.8903766844027491\n",
      "saved as models/model2264.pt\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "lr = 0.007\n",
    "\n",
    "model = FluxAnomalyPredictionTF(20, 0.15, bn=False, features=3, residual=0).to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "progress_bar = tqdm(total=EPOCHS, desc=\"Training Progress\")\n",
    "\n",
    "trainloss = []\n",
    "validloss = []\n",
    "\n",
    "nullts = []\n",
    "novats = []\n",
    "pulsatingts = []\n",
    "transitts = []\n",
    "accuracyts = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "  epoch_loss = []\n",
    "  valid_loss = []\n",
    "\n",
    "  null_correct = 0\n",
    "  nova_correct = 0\n",
    "  pulsating_correct = 0\n",
    "  transit_correct = 0\n",
    "  correct = 0\n",
    "\n",
    "\n",
    "  novas = 0\n",
    "  pulsators = 0\n",
    "  transits = 0\n",
    "  nulls = 0\n",
    "  exs = 0\n",
    "\n",
    "  for data, label in train:\n",
    "\n",
    "    model.train()\n",
    "    out = model(data)\n",
    "\n",
    "    loss = loss_fn(out, label)\n",
    "\n",
    "    epoch_loss.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "\n",
    "  for data, label in valid:\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    loss = loss_fn(out, label)\n",
    "    valid_loss.append(loss.item())\n",
    "    i = torch.argmax(out, dim=1).cpu()\n",
    "    j = torch.argmax(label, dim=1).cpu()\n",
    "\n",
    "    for idx, jdx in zip(i,j):\n",
    "      exs += 1\n",
    "      if idx == jdx:\n",
    "        correct += 1\n",
    "\n",
    "      if jdx == 0:\n",
    "        nulls += 1\n",
    "        if idx == jdx:\n",
    "          null_correct += 1\n",
    "\n",
    "      if jdx == 1:\n",
    "        novas += 1\n",
    "        if idx == jdx:\n",
    "          nova_correct +=1\n",
    "      if jdx == 2:\n",
    "        pulsators += 1\n",
    "        if idx == jdx:\n",
    "          pulsating_correct +=1\n",
    "      if jdx == 3:\n",
    "        transits += 1\n",
    "        if idx == jdx:\n",
    "          transit_correct +=1\n",
    "\n",
    "  training_loss_epoch = np.mean(epoch_loss)\n",
    "  validation_loss_epoch = np.mean(valid_loss)\n",
    "\n",
    "  nullac = null_correct / (nulls + 0.000001)\n",
    "  novacc = nova_correct / (novas + 0.000001)\n",
    "  pulsatoracc = pulsating_correct / (pulsators + 0.00001)\n",
    "  transitacc = transit_correct / (transits + 0.00001)\n",
    "  accuracy = correct / exs\n",
    "\n",
    "  trainloss.append(training_loss_epoch)\n",
    "  validloss.append(validation_loss_epoch)\n",
    "\n",
    "  nullts.append(nullac)\n",
    "  novats.append(novacc)\n",
    "  pulsatingts.append(pulsatoracc)\n",
    "  transitts.append(transitacc)\n",
    "  accuracyts.append(accuracy)\n",
    "\n",
    "\n",
    "  progress_bar.update(1)\n",
    "  p = getprogressplot(trainloss, validloss, accuracyts, nullts, novats, pulsatingts, transitts, EPOCHS, e)\n",
    "  clear_output(wait=False)\n",
    "  display(p)\n",
    "\n",
    "  print(\"Epoch \", e, \": \", training_loss_epoch)\n",
    "  print(\"nulls:\", nullac, \"novas: \", novacc, \"pulsators: \", pulsatoracc, \"transits: \", transitacc)\n",
    "\n",
    "x = range(EPOCHS)\n",
    "\n",
    "p = getprogressplot(trainloss, validloss, accuracyts, nullts, novats, pulsatingts, transitts, EPOCHS, e)\n",
    "clear_output(wait=True)\n",
    "display(p)\n",
    "print(\"Epoch \", e, \": \", training_loss_epoch)\n",
    "\n",
    "savestr = \"models/model\" + str(np.random.randint(1111,9999)) + \".pt\"\n",
    "with open(ROOT + savestr, \"wb\") as f:\n",
    "  torch.save(model.state_dict(),f)\n",
    "\n",
    "print(\"saved as {}\".format(savestr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.007)\n",
    "compete([(\"TF\", model, optim)], 150, train, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qPbsgAA7OIXE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-11-20 21:32:47 73762:31109620 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-11-20 21:32:55 73762:31109620 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-11-20 21:32:55 73762:31109620 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                     dd         4.75%     194.411ms       100.00%        4.093s        4.093s             1  \n",
      "                                            aten::empty         0.11%       4.442ms         0.11%       4.442ms       0.429us         10346  \n",
      "                                          aten::random_         0.00%       5.000us         0.00%       5.000us       2.500us             2  \n",
      "                                             aten::item         0.00%       6.000us         0.00%       7.000us       3.500us             2  \n",
      "                              aten::_local_scalar_dense         0.00%       1.000us         0.00%       1.000us       0.500us             2  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         0.01%     456.000us         0.15%       6.232ms       6.232ms             1  \n",
      "                                         aten::randperm         0.00%      20.000us         0.00%      35.000us      17.500us             2  \n",
      "                                    aten::scalar_tensor         0.00%       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                                          aten::resize_         0.00%       5.000us         0.00%       5.000us       0.833us             6  \n",
      "                                     aten::resolve_conj         0.00%     159.000us         0.00%     159.000us       0.000us        666008  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.093s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = FluxAnomalyPredictionTF(32, 0.1).to(device)\n",
    "\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"dd\"):\n",
    "        ex = next(iter(train))\n",
    "        out = model(ex[0])\n",
    "        loss = loss_fn(out, ex[1])\n",
    "        loss.backward()\n",
    "        # for name, param in model.cpu().named_parameters():\n",
    "        #   print(name, param.grad)\n",
    "        # plot_grad_flow(model.cpu().named_parameters())\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPn/w4j+IUa3ZgM7or7h7ji",
   "gpuType": "T4",
   "mount_file_id": "18k-3VG5eHIKWuvn4cbLSEYkNPYsG3W-J",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02705c67d19b4eb188b228046fbf1e99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26f8b4836e9d4aa3ab3fe425455cb365": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3df46f544a194a80908e44bf1d394491": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bcf7e6f9cd2451687e1ba12c74240e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5af5e5934a9c441784a1791a818e3e76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c2d716c0c134147aa3ca644a3983928": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a4d438c1bef4707babe8b0293c437ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3df46f544a194a80908e44bf1d394491",
      "placeholder": "",
      "style": "IPY_MODEL_5c2d716c0c134147aa3ca644a3983928",
      "value": " 0/720 [00:00&lt;?, ?it/s]"
     }
    },
    "8d345de5fbd84261a1c8588337db694d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9137e576ff0948b6a5bbef5a146e05da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e06b6fd9a99b4bc283bc942d7517086b",
       "IPY_MODEL_f67d8e102ab84d37a7ed583406539773",
       "IPY_MODEL_7a4d438c1bef4707babe8b0293c437ae"
      ],
      "layout": "IPY_MODEL_02705c67d19b4eb188b228046fbf1e99"
     }
    },
    "e06b6fd9a99b4bc283bc942d7517086b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bcf7e6f9cd2451687e1ba12c74240e5",
      "placeholder": "",
      "style": "IPY_MODEL_8d345de5fbd84261a1c8588337db694d",
      "value": "Training Progress:   0%"
     }
    },
    "f67d8e102ab84d37a7ed583406539773": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5af5e5934a9c441784a1791a818e3e76",
      "max": 720,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26f8b4836e9d4aa3ab3fe425455cb365",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
